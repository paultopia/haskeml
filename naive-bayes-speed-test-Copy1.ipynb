{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Naive Bayes Speed Test  OR: Vectorize All the Things!!\n",
    "\n",
    "It turns out that even fairly simple algorithms like Bernoulli naive Bayes can have dramatic speed differences. Let's demonstrate the difference between a naive (naive naive?!) iterative version in Python (with the help... or possibly hinderance... of Pandas) and a vectorific version that leverages linear algebra.  \n",
    "\n",
    "So here's the setup.  You've got a spam dataset with roughly 10k words and 5k observations in it, and you want to fit a straightforward Bernoilli naive Bayes to it. This has gotta be the easiest algorithm in all of machine learning, it really is just \"estimate the priorities as simple proportions, then slap Bayes rule on it.\" \n",
    "\n",
    "Let's start by loading our (pre-prepared) data and the libraries we'll need."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>LABEL</th>\n",
       "      <th>ibh</th>\n",
       "      <th>national</th>\n",
       "      <th>beside</th>\n",
       "      <th>geting</th>\n",
       "      <th>keen</th>\n",
       "      <th>draw</th>\n",
       "      <th>86021</th>\n",
       "      <th>09061744553</th>\n",
       "      <th>weathers</th>\n",
       "      <th>...</th>\n",
       "      <th>un</th>\n",
       "      <th>runs</th>\n",
       "      <th>chrgd50p</th>\n",
       "      <th>dock</th>\n",
       "      <th>brainy</th>\n",
       "      <th>ages</th>\n",
       "      <th>drugdealer</th>\n",
       "      <th>shore</th>\n",
       "      <th>chuckin</th>\n",
       "      <th>dd</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows Ã— 9664 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   LABEL  ibh  national  beside  geting  keen  draw  86021  09061744553  \\\n",
       "0      0    0         0       0       0     0     0      0            0   \n",
       "1      0    0         0       0       0     0     0      0            0   \n",
       "2      1    0         0       0       0     0     0      0            0   \n",
       "3      0    0         0       0       0     0     0      0            0   \n",
       "4      0    0         0       0       0     0     0      0            0   \n",
       "\n",
       "   weathers ...  un  runs  chrgd50p  dock  brainy  ages  drugdealer  shore  \\\n",
       "0         0 ...   0     0         0     0       0     0           0      0   \n",
       "1         0 ...   0     0         0     0       0     0           0      0   \n",
       "2         0 ...   0     0         0     0       0     0           0      0   \n",
       "3         0 ...   0     0         0     0       0     0           0      0   \n",
       "4         0 ...   0     0         0     0       0     0           0      0   \n",
       "\n",
       "   chuckin  dd  \n",
       "0        0   0  \n",
       "1        0   0  \n",
       "2        0   0  \n",
       "3        0   0  \n",
       "4        0   0  \n",
       "\n",
       "[5 rows x 9664 columns]"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import toolz\n",
    "from math import log\n",
    "import cProfile\n",
    "df = pd.read_csv(\"spam_binarized.csv\", index_col=0)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So the first column is our label; the rest are our features.  (Note that it's not super aggressively cleaned, I didn't bother to remove numbers from the dictionary, for example---but for a spam classifier, I don't think that's necessarily bad; it's easy to imagine that a number of spam messages contain \"800\" or other phone number prefixes, for example.)\n",
    "\n",
    "The first task is to calculate the prior probabilities, which is pretty easy.  We'll just want the proportion of true label values, as well as the conditional probabilities of each feature on both label=1 and label=0.  Since everything is an int in {1, 0}, this is super easy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "prob_y = df[\"LABEL\"].sum() / len(df)\n",
    "spams = df[df[\"LABEL\"] == 1]\n",
    "notspams = df[df[\"LABEL\"] == 0]\n",
    "features = list(df)[1:]\n",
    "\n",
    "def conditional_probability_dict(column_label, condition_df):\n",
    "    numerator = condition_df[column_label].sum() + 1 # using laplace smoothing\n",
    "    denominator = len(condition_df) + 2\n",
    "    return {column_label: numerator / denominator}\n",
    "\n",
    "x_probs_conditional_on_spam=[conditional_probability_dict(x, spams) for x in list(spams)[1:]]\n",
    "x_spam_lookup = toolz.merge(x_probs_conditional_on_spam)\n",
    "\n",
    "x_probs_conditional_on_notspam=[conditional_probability_dict(x, notspams) for x in list(notspams)[1:]]\n",
    "x_notspam_lookup = toolz.merge(x_probs_conditional_on_notspam)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ok, now we have all the basic information we need to use our model. In order to generate a prediction on an observation, we follow: \n",
    "\n",
    "$$p(y=\\phi | x_{1..n}) = p(y=\\phi)\\prod_{i=1}^{n}p(x_i | y=\\phi)$$ \n",
    "\n",
    "After calculating that for both the true and the false cases ($\\phi = 1$ and $\\phi = 0$), we simply take the max ($argmag_\\phi$) and assign our prediction for that document to that case.\n",
    "\n",
    "(Note that we drop the denominator in the application of Bayes rule because it's the same for both the true and the false cases, so can't affect the comparision.)\n",
    "\n",
    "Let's start with the naive, slow version. What we're going to do is iterate over all the rows, and, in each row, iterate over the cells.  For each cell, we're going to look up the prior and multiply it out. \n",
    "\n",
    "There's one more refinement we have to start with (and which will become important later).  Because the priors are going to get very, very small, in order to avoid ugliness with floating point errors, we should leverage the fact that $log(xy)=log(x)+log(y)$ and instead calculate $log(p(y=\\phi)) + \\sum_{i=1}^{n}log(p(x_i | y=\\phi))$. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def slow_predict(row):\n",
    "    prob_spam = log(prob_y)\n",
    "    prob_notspam = log(1 - prob_y)\n",
    "    for feat in features:\n",
    "        if row[feat] == 1:\n",
    "            prob_spam = prob_spam + log(x_spam_lookup[feat])\n",
    "            prob_notspam = prob_notspam + log(x_notspam_lookup[feat])\n",
    "    if prob_spam >= prob_notspam:\n",
    "        return 1\n",
    "    else:\n",
    "        return 0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we'll profile this.  Because I know in advance that this will take way too long on the whole dataset, I'll just predict the first, say, 200 rows. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "         63786379 function calls (61853363 primitive calls) in 37.393 seconds\n",
      "\n",
      "   Ordered by: standard name\n",
      "\n",
      "   ncalls  tottime  percall  cumtime  percall filename:lineno(function)\n",
      "        6    0.000    0.000    0.000    0.000 <frozen importlib._bootstrap>:996(_handle_fromlist)\n",
      "      200    1.797    0.009   37.336    0.187 <ipython-input-3-c73fd603c224>:1(slow_predict)\n",
      "        1    0.000    0.000   37.393   37.393 <string>:1(<module>)\n",
      "        1    0.000    0.000    0.000    0.000 __init__.py:177(iteritems)\n",
      "        1    0.000    0.000    0.000    0.000 _methods.py:37(_any)\n",
      "        1    0.000    0.000    0.000    0.000 base.py:1152(_has_complex_internals)\n",
      "  1932600    2.297    0.000    6.696    0.000 base.py:1303(_convert_scalar_indexer)\n",
      "        1    0.000    0.000    0.000    0.000 base.py:1351(_convert_slice_indexer)\n",
      "        1    0.000    0.000    0.000    0.000 base.py:1672(__getitem__)\n",
      "  1932600    5.715    0.000   31.163    0.000 base.py:2454(get_value)\n",
      "        3    0.000    0.000    0.000    0.000 base.py:3375(_validate_indexer)\n",
      "        1    0.000    0.000    0.000    0.000 base.py:367(_simple_new)\n",
      "      405    0.000    0.000    0.000    0.000 base.py:3999(_ensure_index)\n",
      "        1    0.000    0.000    0.000    0.000 base.py:404(_shallow_copy)\n",
      "        1    0.000    0.000    0.000    0.000 base.py:522(_reset_identity)\n",
      "  1933015    0.702    0.000    0.968    0.000 base.py:528(__len__)\n",
      "        2    0.000    0.000    0.000    0.000 base.py:559(values)\n",
      "        1    0.000    0.000    0.000    0.000 base.py:701(_get_attributes_dict)\n",
      "        1    0.000    0.000    0.000    0.000 base.py:703(<listcomp>)\n",
      "        3    0.000    0.000    0.000    0.000 cast.py:759(maybe_castable)\n",
      "        2    0.000    0.000    0.000    0.000 cast.py:771(maybe_infer_to_datetimelike)\n",
      "        3    0.000    0.000    0.000    0.000 cast.py:868(maybe_cast_to_datetime)\n",
      "        4    0.000    0.000    0.000    0.000 common.py:117(is_sparse)\n",
      "        1    0.000    0.000    0.000    0.000 common.py:1453(is_float_dtype)\n",
      "        4    0.000    0.000    0.000    0.000 common.py:1546(is_extension_type)\n",
      "        6    0.000    0.000    0.000    0.000 common.py:1719(_get_dtype_type)\n",
      "        1    0.000    0.000    0.000    0.000 common.py:182(is_bool_indexer)\n",
      "        2    0.000    0.000    0.000    0.000 common.py:1849(pandas_dtype)\n",
      "        4    0.000    0.000    0.000    0.000 common.py:190(is_categorical)\n",
      "        4    0.000    0.000    0.000    0.000 common.py:222(is_datetimetz)\n",
      "        2    0.000    0.000    0.000    0.000 common.py:296(is_datetime64_dtype)\n",
      "        5    0.000    0.000    0.000    0.000 common.py:333(is_datetime64tz_dtype)\n",
      "        1    0.000    0.000    0.000    0.000 common.py:371(is_timedelta64_dtype)\n",
      "  1932601    0.658    0.000    0.837    0.000 common.py:435(_apply_if_callable)\n",
      "        4    0.000    0.000    0.000    0.000 common.py:472(is_categorical_dtype)\n",
      "        3    0.000    0.000    0.000    0.000 common.py:84(is_object_dtype)\n",
      "        9    0.000    0.000    0.000    0.000 dtypes.py:84(is_dtype)\n",
      "        1    0.000    0.000    0.000    0.000 frame.py:250(_constructor)\n",
      "        1    0.000    0.000    0.000    0.000 frame.py:261(__init__)\n",
      "        1    0.000    0.000   37.393   37.393 frame.py:4159(apply)\n",
      "        1    0.000    0.000   37.393   37.393 frame.py:4292(_apply_standard)\n",
      "        1    0.000    0.000    0.000    0.000 frame.py:493(shape)\n",
      "        1    0.000    0.000    0.000    0.000 frame.py:5219(_get_agg_axis)\n",
      "        4    0.000    0.000    0.000    0.000 generic.py:117(__init__)\n",
      "        1    0.000    0.000    0.000    0.000 generic.py:145(_validate_dtype)\n",
      "        1    0.000    0.000    0.000    0.000 generic.py:159(_init_mgr)\n",
      "        1    0.000    0.000    0.000    0.000 generic.py:1607(_indexer)\n",
      "        1    0.000    0.000    0.001    0.001 generic.py:1742(_slice)\n",
      "        1    0.000    0.000    0.000    0.000 generic.py:1762(_set_is_copy)\n",
      "        1    0.000    0.000    0.001    0.001 generic.py:2729(head)\n",
      "        1    0.000    0.000    0.000    0.000 generic.py:3050(__finalize__)\n",
      "        3    0.000    0.000    0.000    0.000 generic.py:3067(__getattr__)\n",
      "        5    0.000    0.000    0.000    0.000 generic.py:3083(__setattr__)\n",
      "        2    0.000    0.000    0.000    0.000 generic.py:3122(_protect_consolidate)\n",
      "        1    0.000    0.000    0.000    0.000 generic.py:3132(_consolidate_inplace)\n",
      "        1    0.000    0.000    0.000    0.000 generic.py:3135(f)\n",
      "        1    0.000    0.000    0.000    0.000 generic.py:3171(_is_mixed_type)\n",
      "        1    0.000    0.000    0.000    0.000 generic.py:3173(<lambda>)\n",
      "        1    0.000    0.000    0.000    0.000 generic.py:3214(as_matrix)\n",
      "        1    0.000    0.000    0.000    0.000 generic.py:3256(values)\n",
      "        2    0.000    0.000    0.000    0.000 generic.py:342(_get_axis_number)\n",
      "        3    0.000    0.000    0.000    0.000 generic.py:355(_get_axis_name)\n",
      "        3    0.000    0.000    0.000    0.000 generic.py:368(_get_axis)\n",
      "        1    0.000    0.000    0.000    0.000 generic.py:372(_get_block_manager_axis)\n",
      "  1932623    0.800    0.000    1.075    0.000 generic.py:7(_check)\n",
      "        1    0.000    0.000    0.001    0.001 indexing.py:1317(__getitem__)\n",
      "        1    0.000    0.000    0.001    0.001 indexing.py:141(_slice)\n",
      "        1    0.000    0.000    0.000    0.000 indexing.py:1583(_has_valid_type)\n",
      "        1    0.000    0.000    0.001    0.001 indexing.py:1689(_get_slice_axis)\n",
      "        1    0.000    0.000    0.001    0.001 indexing.py:1720(_getitem_axis)\n",
      "        1    0.000    0.000    0.000    0.000 indexing.py:2042(need_slice)\n",
      "        1    0.000    0.000    0.000    0.000 indexing.py:238(_convert_slice_indexer)\n",
      "        2    0.000    0.000    0.000    0.000 inference.py:233(is_list_like)\n",
      "      201    0.000    0.000    0.000    0.000 inference.py:364(is_hashable)\n",
      "        4    0.000    0.000    0.000    0.000 internals.py:102(__init__)\n",
      "        2    0.000    0.000    0.000    0.000 internals.py:150(external_values)\n",
      "  1932600    0.336    0.000    0.336    0.000 internals.py:154(internal_values)\n",
      "        1    0.000    0.000    0.000    0.000 internals.py:160(get_values)\n",
      "  1932600    0.767    0.000    1.469    0.000 internals.py:169(to_dense)\n",
      "        8    0.000    0.000    0.000    0.000 internals.py:185(mgr_locs)\n",
      "        1    0.000    0.000    0.000    0.000 internals.py:214(make_block_same_class)\n",
      "        4    0.000    0.000    0.000    0.000 internals.py:222(mgr_locs)\n",
      "        1    0.000    0.000    0.000    0.000 internals.py:256(_slice)\n",
      "        4    0.000    0.000    0.000    0.000 internals.py:2683(make_block)\n",
      "        1    0.000    0.000    0.000    0.000 internals.py:275(getitem_block)\n",
      "        1    0.000    0.000    0.000    0.000 internals.py:2779(__init__)\n",
      "        1    0.000    0.000    0.000    0.000 internals.py:2780(<listcomp>)\n",
      "        2    0.000    0.000    0.000    0.000 internals.py:2820(shape)\n",
      "        6    0.000    0.000    0.000    0.000 internals.py:2822(<genexpr>)\n",
      "        3    0.000    0.000    0.000    0.000 internals.py:2824(ndim)\n",
      "      200    0.001    0.000    0.001    0.000 internals.py:2828(set_axis)\n",
      "        1    0.000    0.000    0.000    0.000 internals.py:2864(_is_single_block)\n",
      "        1    0.000    0.000    0.000    0.000 internals.py:2876(_rebuild_blknos_and_blklocs)\n",
      "        1    0.000    0.000    0.000    0.000 internals.py:2897(_get_items)\n",
      "        1    0.000    0.000    0.000    0.000 internals.py:2986(__len__)\n",
      "  1932601    0.489    0.000    0.489    0.000 internals.py:303(dtype)\n",
      "        1    0.000    0.000    0.000    0.000 internals.py:307(ftype)\n",
      "        3    0.000    0.000    0.000    0.000 internals.py:3296(is_consolidated)\n",
      "        1    0.000    0.000    0.000    0.000 internals.py:3304(_consolidate_check)\n",
      "        1    0.000    0.000    0.000    0.000 internals.py:3305(<listcomp>)\n",
      "        1    0.000    0.000    0.000    0.000 internals.py:3309(is_mixed_type)\n",
      "        1    0.000    0.000    0.001    0.001 internals.py:3384(get_slice)\n",
      "        1    0.000    0.000    0.000    0.000 internals.py:3394(<listcomp>)\n",
      "        1    0.000    0.000    0.000    0.000 internals.py:3438(as_matrix)\n",
      "        1    0.000    0.000    0.000    0.000 internals.py:3560(consolidate)\n",
      "        2    0.000    0.000    0.000    0.000 internals.py:3576(_consolidate_inplace)\n",
      "        3    0.000    0.000    0.000    0.000 internals.py:4078(__init__)\n",
      "  5798002    1.153    0.000    1.153    0.000 internals.py:4124(_block)\n",
      "  1932600    1.384    0.000    2.261    0.000 internals.py:4194(dtype)\n",
      "        2    0.000    0.000    0.000    0.000 internals.py:4218(external_values)\n",
      "  1932600    1.399    0.000    2.133    0.000 internals.py:4221(internal_values)\n",
      "  1932600    2.434    0.000    4.605    0.000 internals.py:4224(get_values)\n",
      "        1    0.000    0.000    0.000    0.000 numeric.py:75(is_all_dates)\n",
      "        3    0.000    0.000    0.001    0.000 series.py:139(__init__)\n",
      "      203    0.001    0.000    0.003    0.000 series.py:284(_set_axis)\n",
      "        3    0.000    0.000    0.000    0.000 series.py:2894(_sanitize_array)\n",
      "        3    0.000    0.000    0.000    0.000 series.py:2911(_try_cast)\n",
      "      203    0.000    0.000    0.000    0.000 series.py:310(_set_subtyp)\n",
      "        3    0.000    0.000    0.000    0.000 series.py:320(name)\n",
      "      203    0.001    0.000    0.001    0.000 series.py:324(name)\n",
      "  1932600    1.083    0.000    3.344    0.000 series.py:331(dtype)\n",
      "        2    0.000    0.000    0.000    0.000 series.py:351(values)\n",
      "  1932600    0.907    0.000    3.040    0.000 series.py:384(_values)\n",
      "  1932600    0.873    0.000    5.478    0.000 series.py:389(get_values)\n",
      "  1932600    3.302    0.000   35.532    0.000 series.py:598(__getitem__)\n",
      "        1    0.000    0.000    0.000    0.000 {built-in method __new__ of type object at 0x10022c440}\n",
      "        1    0.000    0.000    0.000    0.000 {built-in method builtins.all}\n",
      "  1932601    0.180    0.000    0.180    0.000 {built-in method builtins.callable}\n",
      "        1    0.000    0.000   37.393   37.393 {built-in method builtins.exec}\n",
      "  5797831    2.430    0.000    5.471    0.000 {built-in method builtins.getattr}\n",
      "       18    0.000    0.000    0.000    0.000 {built-in method builtins.hasattr}\n",
      "      201    0.000    0.000    0.000    0.000 {built-in method builtins.hash}\n",
      "  3865723    1.575    0.000    2.649    0.000 {built-in method builtins.isinstance}\n",
      "       26    0.000    0.000    0.000    0.000 {built-in method builtins.issubclass}\n",
      "        1    0.000    0.000    0.000    0.000 {built-in method builtins.iter}\n",
      "3866055/1933039    1.233    0.000    1.936    0.000 {built-in method builtins.len}\n",
      "        1    0.000    0.000    0.000    0.000 {built-in method builtins.min}\n",
      "     6424    0.007    0.000    0.007    0.000 {built-in method math.log}\n",
      "        1    0.000    0.000    0.000    0.000 {built-in method numpy.core.multiarray.arange}\n",
      "  1932606    0.334    0.000    0.334    0.000 {built-in method numpy.core.multiarray.array}\n",
      "        3    0.000    0.000    0.000    0.000 {built-in method numpy.core.multiarray.empty}\n",
      "  1932600    0.229    0.000    0.229    0.000 {built-in method pandas._libs.lib.is_float}\n",
      "        3    0.000    0.000    0.000    0.000 {built-in method pandas._libs.lib.is_integer}\n",
      "  1932601    0.230    0.000    0.230    0.000 {built-in method pandas._libs.lib.isscalar}\n",
      "        1    0.000    0.000    0.000    0.000 {method 'any' of 'numpy.ndarray' objects}\n",
      "        1    0.000    0.000    0.000    0.000 {method 'disable' of '_lsprof.Profiler' objects}\n",
      "        2    0.000    0.000    0.000    0.000 {method 'fill' of 'numpy.ndarray' objects}\n",
      "        5    0.000    0.000    0.000    0.000 {method 'get' of 'dict' objects}\n",
      "  1932600    2.249    0.000    2.249    0.000 {method 'get_value' of 'pandas._libs.index.IndexEngine' objects}\n",
      "        2    0.000    0.000    0.000    0.000 {method 'items' of 'dict' objects}\n",
      "        1    0.000    0.000    0.000    0.000 {method 'pop' of 'dict' objects}\n",
      "        1    0.000    0.000    0.000    0.000 {method 'reduce' of 'numpy.ufunc' objects}\n",
      "        1    0.000    0.000    0.000    0.000 {method 'update' of 'dict' objects}\n",
      "  1932602    0.702    0.000    0.702    0.000 {method 'view' of 'numpy.ndarray' objects}\n",
      "        1    0.052    0.052   37.392   37.392 {pandas._libs.lib.reduce}\n",
      "  3865200    2.070    0.000    7.548    0.000 {pandas._libs.lib.values_from_object}\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "cProfile.run(\"df.head(200).apply(slow_predict, axis = 1)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So that took a while.  38 seconds for 200 rows -> about 15 minutes for 5k rows.  That's totally unreasonable. \n",
    "\n",
    "Can we do better?  The key is to observe that once we start adding up logs, the calculation of the posterior on $y=\\phi$ for each row is nothing more than the dot product of the log priors for the features and the matrix containing each of those features, plus adding $log(p(y=\\phi))$ to each.  (Conveniently, if the word is present for a row, the feature is 1, and the log prior gets included in the dot product; if the word is absent, the dot product multiplies by zero, so there's no need for a conditional to check to see if it's in there.)\n",
    "\n",
    "For ease of prediction, let's reshape the data a little bit to start.  We'll split out our features from the labels for ease of multiplication, and add a column of all ones at the start to facilitate including $log(p(y=\\phi))$ in the dot product rather than adding it later. We'll also change our feature priors from a lookup dict to a Pandas series so that we can multiply it out."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "clean_spamdf = df.iloc[:,1:] # get rid of the labels column\n",
    "clean_spamdf.insert(0, \"pseudo_intercept\", 1) \n",
    "# add column to get posterior on y into dot product.\n",
    "\n",
    "true_vector =[log(x_spam_lookup[x]) for x in features]\n",
    "true_vector.insert(0, log(prob_y))\n",
    "true_vector = pd.Series(true_vector) \n",
    "# probably could just keep this as a list, but Pandas series is built on top of \n",
    "# numpy anyway, so why not make it clean?\n",
    "\n",
    "false_vector =[log(x_notspam_lookup[x]) for x in features]\n",
    "false_vector.insert(0, log(1-prob_y))\n",
    "false_vector = pd.Series(false_vector)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ok, as before, we'll predict 200 rows.  For the actual comparison, we *could* just loop over the two dot products, but since we're using numpy for everything, we can just leverage array transformations and comparisons to do it directly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "         417 function calls (401 primitive calls) in 0.110 seconds\n",
      "\n",
      "   Ordered by: standard name\n",
      "\n",
      "   ncalls  tottime  percall  cumtime  percall filename:lineno(function)\n",
      "        1    0.000    0.000    0.092    0.092 <ipython-input-6-7b88fdccb39e>:1(fast_predict)\n",
      "        1    0.002    0.002    0.110    0.110 <string>:1(<module>)\n",
      "        1    0.000    0.000    0.000    0.000 __init__.py:177(iteritems)\n",
      "        2    0.000    0.000    0.000    0.000 _methods.py:37(_any)\n",
      "        1    0.000    0.000    0.000    0.000 base.py:1351(_convert_slice_indexer)\n",
      "        1    0.000    0.000    0.000    0.000 base.py:1672(__getitem__)\n",
      "        3    0.000    0.000    0.000    0.000 base.py:3375(_validate_indexer)\n",
      "        1    0.000    0.000    0.000    0.000 base.py:367(_simple_new)\n",
      "        2    0.000    0.000    0.000    0.000 base.py:3999(_ensure_index)\n",
      "        1    0.000    0.000    0.000    0.000 base.py:404(_shallow_copy)\n",
      "        1    0.000    0.000    0.000    0.000 base.py:522(_reset_identity)\n",
      "       12    0.000    0.000    0.000    0.000 base.py:528(__len__)\n",
      "        1    0.000    0.000    0.000    0.000 base.py:701(_get_attributes_dict)\n",
      "        1    0.000    0.000    0.000    0.000 base.py:703(<listcomp>)\n",
      "        1    0.000    0.000    0.000    0.000 common.py:182(is_bool_indexer)\n",
      "        1    0.000    0.000    0.000    0.000 common.py:435(_apply_if_callable)\n",
      "        4    0.000    0.000    0.000    0.000 common.py:84(is_object_dtype)\n",
      "        1    0.000    0.000    0.000    0.000 frame.py:250(_constructor)\n",
      "        1    0.000    0.000    0.000    0.000 frame.py:261(__init__)\n",
      "        2    0.000    0.000    0.000    0.000 fromnumeric.py:55(_wrapfunc)\n",
      "        1    0.000    0.000    0.000    0.000 fromnumeric.py:826(argsort)\n",
      "        1    0.000    0.000    0.000    0.000 fromnumeric.py:910(argmax)\n",
      "        1    0.000    0.000    0.000    0.000 generic.py:117(__init__)\n",
      "        1    0.000    0.000    0.000    0.000 generic.py:159(_init_mgr)\n",
      "        1    0.000    0.000    0.000    0.000 generic.py:1607(_indexer)\n",
      "        1    0.000    0.000    0.016    0.016 generic.py:1742(_slice)\n",
      "        1    0.000    0.000    0.000    0.000 generic.py:1762(_set_is_copy)\n",
      "        1    0.000    0.000    0.016    0.016 generic.py:2729(head)\n",
      "        1    0.000    0.000    0.000    0.000 generic.py:3050(__finalize__)\n",
      "       16    0.000    0.000    0.000    0.000 generic.py:3067(__getattr__)\n",
      "        6    0.000    0.000    0.000    0.000 generic.py:3083(__setattr__)\n",
      "        4    0.000    0.000    0.000    0.000 generic.py:3122(_protect_consolidate)\n",
      "        4    0.000    0.000    0.000    0.000 generic.py:3132(_consolidate_inplace)\n",
      "        4    0.000    0.000    0.000    0.000 generic.py:3135(f)\n",
      "        4    0.000    0.000    0.000    0.000 generic.py:3214(as_matrix)\n",
      "        4    0.000    0.000    0.000    0.000 generic.py:3284(get_values)\n",
      "        1    0.000    0.000    0.000    0.000 generic.py:342(_get_axis_number)\n",
      "        1    0.000    0.000    0.000    0.000 generic.py:355(_get_axis_name)\n",
      "        1    0.000    0.000    0.000    0.000 generic.py:368(_get_axis)\n",
      "        1    0.000    0.000    0.000    0.000 generic.py:372(_get_block_manager_axis)\n",
      "        1    0.000    0.000    0.000    0.000 generic.py:436(ndim)\n",
      "        1    0.000    0.000    0.000    0.000 generic.py:7(_check)\n",
      "        4    0.000    0.000    0.000    0.000 generic.py:984(__array__)\n",
      "        1    0.000    0.000    0.016    0.016 indexing.py:1317(__getitem__)\n",
      "        1    0.000    0.000    0.016    0.016 indexing.py:141(_slice)\n",
      "        1    0.000    0.000    0.000    0.000 indexing.py:1583(_has_valid_type)\n",
      "        1    0.000    0.000    0.016    0.016 indexing.py:1689(_get_slice_axis)\n",
      "        1    0.000    0.000    0.016    0.016 indexing.py:1720(_getitem_axis)\n",
      "        1    0.000    0.000    0.000    0.000 indexing.py:2042(need_slice)\n",
      "        1    0.000    0.000    0.000    0.000 indexing.py:238(_convert_slice_indexer)\n",
      "        1    0.000    0.000    0.000    0.000 indexing.py:93(__init__)\n",
      "        3    0.000    0.000    0.000    0.000 internals.py:102(__init__)\n",
      "        4    0.000    0.000    0.000    0.000 internals.py:117(_consolidate_key)\n",
      "        4    0.000    0.000    0.000    0.000 internals.py:160(get_values)\n",
      "        4    0.000    0.000    0.000    0.000 internals.py:169(to_dense)\n",
      "       18    0.000    0.000    0.000    0.000 internals.py:185(mgr_locs)\n",
      "        2    0.000    0.000    0.000    0.000 internals.py:214(make_block_same_class)\n",
      "        3    0.000    0.000    0.000    0.000 internals.py:222(mgr_locs)\n",
      "        2    0.000    0.000    0.000    0.000 internals.py:256(_slice)\n",
      "        3    0.000    0.000    0.000    0.000 internals.py:2683(make_block)\n",
      "        2    0.000    0.000    0.000    0.000 internals.py:275(getitem_block)\n",
      "        1    0.000    0.000    0.001    0.001 internals.py:2779(__init__)\n",
      "        1    0.000    0.000    0.000    0.000 internals.py:2780(<listcomp>)\n",
      "        4    0.000    0.000    0.000    0.000 internals.py:2820(shape)\n",
      "       12    0.000    0.000    0.000    0.000 internals.py:2822(<genexpr>)\n",
      "        8    0.000    0.000    0.000    0.000 internals.py:2824(ndim)\n",
      "        4    0.000    0.000    0.000    0.000 internals.py:2864(_is_single_block)\n",
      "        2    0.001    0.000    0.001    0.001 internals.py:2876(_rebuild_blknos_and_blklocs)\n",
      "        4    0.000    0.000    0.000    0.000 internals.py:2897(_get_items)\n",
      "        4    0.000    0.000    0.000    0.000 internals.py:2986(__len__)\n",
      "        6    0.000    0.000    0.000    0.000 internals.py:303(dtype)\n",
      "        2    0.000    0.000    0.000    0.000 internals.py:307(ftype)\n",
      "        5    0.000    0.000    0.000    0.000 internals.py:3296(is_consolidated)\n",
      "        1    0.000    0.000    0.000    0.000 internals.py:3304(_consolidate_check)\n",
      "        1    0.000    0.000    0.000    0.000 internals.py:3305(<listcomp>)\n",
      "        1    0.000    0.000    0.016    0.016 internals.py:3384(get_slice)\n",
      "        1    0.000    0.000    0.000    0.000 internals.py:3394(<listcomp>)\n",
      "        4    0.000    0.000    0.000    0.000 internals.py:3438(as_matrix)\n",
      "        4    0.000    0.000    0.000    0.000 internals.py:3560(consolidate)\n",
      "        1    0.000    0.000    0.014    0.014 internals.py:3576(_consolidate_inplace)\n",
      "        4    0.000    0.000    0.000    0.000 internals.py:4124(_block)\n",
      "        4    0.000    0.000    0.000    0.000 internals.py:4224(get_values)\n",
      "        1    0.000    0.000    0.014    0.014 internals.py:4513(_consolidate)\n",
      "        4    0.000    0.000    0.000    0.000 internals.py:4519(<lambda>)\n",
      "        1    0.010    0.010    0.014    0.014 internals.py:4530(_merge_blocks)\n",
      "        1    0.000    0.000    0.000    0.000 internals.py:4544(<listcomp>)\n",
      "        1    0.000    0.000    0.000    0.000 internals.py:4545(<listcomp>)\n",
      "        1    0.000    0.000    0.000    0.000 internals.py:4557(_extend_blocks)\n",
      "        1    0.000    0.000    0.004    0.004 internals.py:4583(_vstack)\n",
      "        2    0.000    0.000    0.000    0.000 numeric.py:534(asanyarray)\n",
      "        1    0.000    0.000    0.000    0.000 numeric.py:660(require)\n",
      "        2    0.000    0.000    0.000    0.000 numeric.py:729(<genexpr>)\n",
      "        4    0.000    0.000    0.000    0.000 series.py:389(get_values)\n",
      "        4    0.000    0.000    0.000    0.000 series.py:471(__array__)\n",
      "        1    0.000    0.000    0.004    0.004 shape_base.py:182(vstack)\n",
      "        1    0.000    0.000    0.000    0.000 shape_base.py:237(<listcomp>)\n",
      "        1    0.000    0.000    0.000    0.000 shape_base.py:314(column_stack)\n",
      "        2    0.000    0.000    0.000    0.000 shape_base.py:63(atleast_2d)\n",
      "        1    0.000    0.000    0.000    0.000 {built-in method __new__ of type object at 0x10022c440}\n",
      "        1    0.000    0.000    0.000    0.000 {built-in method builtins.callable}\n",
      "        1    0.000    0.000    0.110    0.110 {built-in method builtins.exec}\n",
      "        6    0.000    0.000    0.000    0.000 {built-in method builtins.getattr}\n",
      "        1    0.000    0.000    0.000    0.000 {built-in method builtins.hasattr}\n",
      "       21    0.000    0.000    0.000    0.000 {built-in method builtins.isinstance}\n",
      "        5    0.000    0.000    0.000    0.000 {built-in method builtins.issubclass}\n",
      "        1    0.000    0.000    0.000    0.000 {built-in method builtins.iter}\n",
      "    66/50    0.000    0.000    0.000    0.000 {built-in method builtins.len}\n",
      "        1    0.000    0.000    0.000    0.000 {built-in method builtins.min}\n",
      "        1    0.000    0.000    0.000    0.000 {built-in method builtins.setattr}\n",
      "        1    0.000    0.000    0.000    0.000 {built-in method builtins.sorted}\n",
      "        3    0.000    0.000    0.000    0.000 {built-in method numpy.core.multiarray.arange}\n",
      "       11    0.000    0.000    0.000    0.000 {built-in method numpy.core.multiarray.array}\n",
      "        3    0.004    0.001    0.004    0.001 {built-in method numpy.core.multiarray.concatenate}\n",
      "        2    0.092    0.046    0.092    0.046 {built-in method numpy.core.multiarray.dot}\n",
      "        4    0.000    0.000    0.000    0.000 {built-in method numpy.core.multiarray.empty}\n",
      "        2    0.000    0.000    0.000    0.000 {built-in method pandas._libs.lib.is_integer}\n",
      "        1    0.000    0.000    0.000    0.000 {built-in method pandas._libs.lib.isscalar}\n",
      "        2    0.000    0.000    0.000    0.000 {method 'any' of 'numpy.ndarray' objects}\n",
      "        5    0.000    0.000    0.000    0.000 {method 'append' of 'list' objects}\n",
      "        1    0.000    0.000    0.000    0.000 {method 'argmax' of 'numpy.ndarray' objects}\n",
      "        1    0.000    0.000    0.000    0.000 {method 'argsort' of 'numpy.ndarray' objects}\n",
      "        1    0.000    0.000    0.000    0.000 {method 'disable' of '_lsprof.Profiler' objects}\n",
      "        4    0.000    0.000    0.000    0.000 {method 'fill' of 'numpy.ndarray' objects}\n",
      "        2    0.000    0.000    0.000    0.000 {method 'get' of 'dict' objects}\n",
      "        2    0.000    0.000    0.000    0.000 {method 'items' of 'dict' objects}\n",
      "        2    0.000    0.000    0.000    0.000 {method 'reduce' of 'numpy.ufunc' objects}\n",
      "        1    0.000    0.000    0.000    0.000 {method 'update' of 'dict' objects}\n",
      "        1    0.000    0.000    0.000    0.000 {method 'upper' of 'str' objects}\n",
      "        4    0.000    0.000    0.000    0.000 {method 'view' of 'numpy.ndarray' objects}\n",
      "        4    0.000    0.000    0.000    0.000 {pandas._libs.lib.values_from_object}\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "def fast_predict(df, true_priors, false_priors):\n",
    "    true_posterior = np.dot(df, true_priors)\n",
    "    false_posterior = np.dot(df, false_priors)\n",
    "    combined = np.column_stack((false_posterior, true_posterior)) # basically a transpose\n",
    "    # reordering so that argmax will return 0 when the first column is max... \n",
    "    return np.argmax(combined, axis=1)\n",
    "\n",
    "cProfile.run(\"fast_predict(clean_spamdf.head(200), true_vector, false_vector)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "0.053 seconds!  That is *7,000 times as fast*!  Let's see how long it takes to cover the whole dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#cProfile.run(\"fast_predict(clean_spamdf, true_vector, false_vector)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "!!!  Just to make sure we're not cheating, let's actually generate those predictions, and see how many of them match the labels."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#predictions = list(fast_predict(clean_spamdf, true_vector, false_vector))\n",
    "#true_labels = list(df[\"LABEL\"])\n",
    "#errors = 0\n",
    "#matches = 0\n",
    "#for idx, item in enumerate(true_labels):\n",
    "#    if predictions[idx] == item:\n",
    "#        matches += 1\n",
    "#    else:\n",
    "#        errors += 1\n",
    "#print(\"correct predictions: \" + str(matches))\n",
    "#print(\"incorrect predictions: \" + str(errors))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "That's not amazing performance (spams are rare enough that we'd get better accuracy just by predicting everything as not spam), but it's enough to make it plausible that naive Bayes is actually happening here, rather than random senseless math.\n",
    "\n",
    "So there you are.  The lesson: *vectorize all the things!!*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0      -2.009803\n",
      "1      -5.520127\n",
      "2      -4.053790\n",
      "3      -6.618739\n",
      "4      -6.618739\n",
      "5      -6.618739\n",
      "6      -2.955177\n",
      "7      -4.672829\n",
      "8      -5.520127\n",
      "9      -6.618739\n",
      "10     -6.618739\n",
      "11     -6.618739\n",
      "12     -5.925592\n",
      "13     -6.618739\n",
      "14     -5.925592\n",
      "15     -6.618739\n",
      "16     -6.618739\n",
      "17     -5.925592\n",
      "18     -6.618739\n",
      "19     -6.618739\n",
      "20     -6.618739\n",
      "21     -6.618739\n",
      "22     -6.618739\n",
      "23     -6.618739\n",
      "24     -5.925592\n",
      "25     -5.520127\n",
      "26     -6.618739\n",
      "27     -6.618739\n",
      "28     -5.925592\n",
      "29     -6.618739\n",
      "          ...   \n",
      "9634   -6.618739\n",
      "9635   -6.618739\n",
      "9636   -6.618739\n",
      "9637   -6.618739\n",
      "9638   -6.618739\n",
      "9639   -6.618739\n",
      "9640   -5.925592\n",
      "9641   -6.618739\n",
      "9642   -5.925592\n",
      "9643   -6.618739\n",
      "9644   -6.618739\n",
      "9645   -6.618739\n",
      "9646   -6.618739\n",
      "9647   -5.925592\n",
      "9648   -5.925592\n",
      "9649   -5.925592\n",
      "9650   -6.618739\n",
      "9651   -5.520127\n",
      "9652   -5.925592\n",
      "9653   -6.618739\n",
      "9654   -6.618739\n",
      "9655   -6.618739\n",
      "9656   -5.925592\n",
      "9657   -6.618739\n",
      "9658   -6.618739\n",
      "9659   -5.925592\n",
      "9660   -6.618739\n",
      "9661   -6.618739\n",
      "9662   -6.618739\n",
      "9663   -6.618739\n",
      "Length: 9664, dtype: float64\n"
     ]
    }
   ],
   "source": [
    "print(true_vector)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
